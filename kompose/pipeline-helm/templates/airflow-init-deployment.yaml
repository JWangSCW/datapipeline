apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    kompose.cmd: kompose convert -v --volumes persistentVolumeClaim --file docker-compose-compatible-kompose.yml
    kompose.version: 1.34.0 (cbf2835db)
  labels:
    io.kompose.service: airflow-init
  name: airflow-init
spec:
  template:
    metadata:
      annotations:
        kompose.cmd: kompose convert -v --volumes persistentVolumeClaim --file docker-compose-compatible-kompose.yml
        kompose.version: 1.34.0 (cbf2835db)
      labels:
        io.kompose.service: airflow-init
    spec:
      containers:
        - args:
            - -c
            - |
              function ver() {
                printf "%04d%04d%04d%04d" ${1//./ }
              }
              airflow_version=$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO airflow version)
              airflow_version_comparable=$(ver "${airflow_version}")
              min_airflow_version=2.2.0
              min_airflow_version_comparable=$(ver "${min_airflow_version}")

              if (( airflow_version_comparable < min_airflow_version_comparable )); then
                echo -e "\033[1;31mERROR!!!: Too old Airflow version ${airflow_version}!\e[0m"
                echo "The minimum Airflow version supported: ${min_airflow_version}. Only use this or higher!"
                exit 1
              fi

              if [[ -z "${AIRFLOW_UID}" ]]; then
                echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
                echo "If you are on Linux, you SHOULD follow the instructions below to set AIRFLOW_UID environment variable."
                echo "For other operating systems, you can get rid of the warning with a manually created .env file."
                echo "See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
              fi

              one_meg=1048576
              mem_available=$(( $(getconf _PHYS_PAGES) * $(getconf PAGE_SIZE) / one_meg ))
              cpus_available=$(grep -cE 'cpu[0-9]+' /proc/stat)
              disk_available=$(df / | tail -1 | awk '{print $4}')

              warning_resources="false"
              if (( mem_available < 4000 )); then
                echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
                echo "At least 4GB of memory required. You have $(numfmt --to=iec $((mem_available * one_meg)))"
                warning_resources="true"
              fi

              if (( cpus_available < 2 )); then
                echo -e "\033[1;33mWARNING!!!: Not enough CPUs available for Docker.\e[0m"
                echo "At least 2 CPUs recommended. You have ${cpus_available}"
                warning_resources="true"
              fi

              if (( disk_available < one_meg * 10 )); then
                echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
                echo "At least 10 GB recommended. You have $(numfmt --to=iec $((disk_available * 1024)))"
                warning_resources="true"
              fi

              if [[ "${warning_resources}" == "true" ]]; then
                echo -e "\033[1;33mWARNING!!!: You do not have enough resources to run Airflow!\e[0m"
              fi
              # mkdir -p /sources/logs /sources/dags /sources/plugins
              # chown -R ":0" /sources/{logs,dags,plugins}
              exec /entrypoint airflow version
          command:
            - /bin/bash
          env:
            - name: AIRFLOW_CONN_SPARK_CONNECTION
              value: spark://spark%3A%2F%2Fspark-master:7077
            - name: AIRFLOW__API__AUTH_BACKENDS
              value: airflow.api.auth.backend.basic_auth
            - name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
              value: "true"
            - name: AIRFLOW__CORE__EXECUTOR
              value: LocalExecutor
            - name: AIRFLOW__CORE__FERNET_KEY
            - name: AIRFLOW__CORE__LOAD_EXAMPLES
              value: "false"
            - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
              value: postgresql+psycopg2://airflow:airflow@postgres/airflow
            - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
              value: postgresql+psycopg2://airflow:airflow@postgres/airflow
            - name: _AIRFLOW_DB_UPGRADE
              value: "true"
            - name: _AIRFLOW_WWW_USER_CREATE
              value: "true"
            - name: _AIRFLOW_WWW_USER_PASSWORD
              value: airflow
            - name: _AIRFLOW_WWW_USER_USERNAME
              value: airflow
            - name: _PIP_ADDITIONAL_REQUIREMENTS
#              value: apache_airflow==2.5.0 pyarrow==9.0.0 pyspark==3.1.3 markdown-it-py==2.1.0 mdit-py-plugins==0.3.1 marshmallow-oneofschema==3.0.1 lazy-object-proxy==1.8.0 jsonschema==4.17.3 Jinja2==3.1.2 itsdangerous==2.1.2 httpcore==0.16.2 gunicorn==20.1.0 httpx==0.23.1 dill==0.3.1.1 rich==12.6.0 cryptography==36.0.2 Werkzeug==2.2.2 croniter==1.3.8 Flask==2.2.2 apache-airflow-providers-apache-spark==4.1.0 pyOpenSSL==22.0.0
          image: rg.fr-par.scw.cloud/namespace-pipeline/airflow-scheduler:2.5
          name: airflow-init
          volumeMounts:
            - name: airflow-init-claim
              mountPath: /sources
      restartPolicy: Never
      volumes:
        - name: airflow-init-claim
          persistentVolumeClaim:
            claimName: airflow-init-claim